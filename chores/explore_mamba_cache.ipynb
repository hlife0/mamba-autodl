{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlife/Mamba-experiment/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state-spaces/mamba-2.8b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hlife/Mamba-experiment/./mamba/mamba_ssm/utils/hf.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(resolved_archive_file, map_location=mapped_device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: 2,768,345,600 parameters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './mamba')\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "MODEL_NAME = \"state-spaces/mamba-2.8b\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = MambaLMHeadModel.from_pretrained(MODEL_NAME, device=DEVICE)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "print(f\"✓ Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trunk 1: torch.Size([1, 10]) tokens\n",
      "Trunk 2: torch.Size([1, 32]) tokens\n"
     ]
    }
   ],
   "source": [
    "# Test texts\n",
    "trunk_1 = \"This paper addresses the challenges of running multiple machine learning\"\n",
    "trunk_2 = \"models on resource-constrained edge devices, which are often equipped with a variety of processors like CPUs, GPUs, and DSPs. The primary goal is\"\n",
    "\n",
    "# Tokenize\n",
    "tokens_1 = tokenizer(trunk_1, return_tensors=\"pt\", return_attention_mask=False)\n",
    "tokens_2 = tokenizer(trunk_2, return_tensors=\"pt\", return_attention_mask=False)\n",
    "input_ids_1 = tokens_1.input_ids.to(DEVICE)\n",
    "input_ids_2 = tokens_2.input_ids.to(DEVICE)\n",
    "\n",
    "print(f\"Trunk 1: {input_ids_1.shape} tokens\")\n",
    "print(f\"Trunk 2: {input_ids_2.shape} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prefill trunk_1 and get cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Prefill trunk_1 ===\n",
      "✓ Processed trunk_1: seqlen_offset=10\n",
      "✓ Cache extracted: 64 layers\n",
      "Sample cache shape: torch.Size([1, 5120, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Prefill trunk_1 ===\")\n",
    "\n",
    "# Initialize cache\n",
    "inference_params = InferenceParams(\n",
    "    max_seqlen=2048, max_batch_size=1, seqlen_offset=0, key_value_memory_dict={}\n",
    ")\n",
    "\n",
    "# Process trunk 1\n",
    "with torch.no_grad():\n",
    "    output_1 = model(input_ids_1, inference_params=inference_params)\n",
    "\n",
    "# Update offset (Mamba requires manual update)\n",
    "inference_params.seqlen_offset += input_ids_1.shape[1]\n",
    "\n",
    "# Extract cache\n",
    "cache_after_1 = {}\n",
    "for layer_idx, (conv_state, ssm_state) in inference_params.key_value_memory_dict.items():\n",
    "    cache_after_1[layer_idx] = {\n",
    "        'conv_state': conv_state.clone().cpu(),\n",
    "        'ssm_state': ssm_state.clone().cpu()\n",
    "    }\n",
    "\n",
    "print(f\"✓ Processed trunk_1: seqlen_offset={inference_params.seqlen_offset}\")\n",
    "print(f\"✓ Cache extracted: {len(cache_after_1)} layers\")\n",
    "print(f\"Sample cache shape: {list(cache_after_1.values())[0]['conv_state'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Continue with trunk_2 from existing cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Continue with trunk_2 ===\n",
      "✓ Processed trunk_2: seqlen_offset=42\n",
      "✓ Final sequence length: 42\n",
      "✓ Output shape: torch.Size([1, 32, 50280])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Continue with trunk_2 ===\")\n",
    "\n",
    "# Save cache before\n",
    "cache_before_2 = {k: {'conv_state': v[0].clone().cpu(), 'ssm_state': v[1].clone().cpu()} \n",
    "                 for k, v in inference_params.key_value_memory_dict.items()}\n",
    "\n",
    "# Process trunk_2 token by token (Mamba requires this with cache)\n",
    "with torch.no_grad():\n",
    "    outputs_2 = []\n",
    "    for i in range(input_ids_2.shape[1]):\n",
    "        step_output = model(input_ids_2[:, i:i+1], inference_params=inference_params)\n",
    "        outputs_2.append(step_output.logits if hasattr(step_output, 'logits') else step_output)\n",
    "        inference_params.seqlen_offset += 1\n",
    "    \n",
    "    output_2 = type('MockOutput', (), {'logits': torch.cat(outputs_2, dim=1)})()\n",
    "\n",
    "print(f\"✓ Processed trunk_2: seqlen_offset={inference_params.seqlen_offset}\")\n",
    "print(f\"✓ Final sequence length: {input_ids_1.shape[1] + input_ids_2.shape[1]}\")\n",
    "print(f\"✓ Output shape: {output_2.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decode from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 3: Decode from cache ===\n",
      "Step 1: token 281 = ' to'\n",
      "Step 6: token 476 = ' can'\n",
      "Step 11: token 4715 = ' learning'\n",
      "Step 16: token 15 = '.'\n",
      "\n",
      "✓ Generated 20 tokens\n",
      "Total decode time: 531.29 ms\n",
      "Time per token: 26.56 ms/token\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 3: Decode from cache ===\")\n",
    "\n",
    "# Generate tokens\n",
    "max_tokens = 20\n",
    "generated_tokens = []\n",
    "current_logits = output_2.logits[:, -1:, :]  # Start with last token\n",
    "current_token = None\n",
    "decode_start_time = time.time()\n",
    "\n",
    "for step in range(max_tokens):\n",
    "    if step == 0:\n",
    "        # First step: get last token from previous output\n",
    "        next_token_logits = output_2.logits[:, -1:, :]\n",
    "    else:\n",
    "        # Subsequent steps: single token forward pass\n",
    "        with torch.no_grad():\n",
    "            # Prepare input in the correct format [batch, seq_len=1]\n",
    "            if current_token.dim() == 0:  # scalar\n",
    "                input_tensor = current_token.unsqueeze(0).unsqueeze(0)\n",
    "            elif current_token.dim() == 1:  # [seq_len]\n",
    "                input_tensor = current_token.unsqueeze(0)\n",
    "            elif current_token.dim() == 2:\n",
    "                if current_token.shape[0] == 1 and current_token.shape[1] == 1:\n",
    "                    input_tensor = current_tensor  # already [1, 1]\n",
    "                else:\n",
    "                    input_tensor = current_tensor[0:1, -1:]  # ensure [1, 1]\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected token shape: {current_token.shape}\")\n",
    "\n",
    "            # Ensure the input has exactly shape [batch_size=1, seq_len=1]\n",
    "            if input_tensor.shape != (1, 1):\n",
    "                input_tensor = input_tensor.reshape(1, 1)\n",
    "\n",
    "            step_output = model(\n",
    "                input_tensor,\n",
    "                inference_params=inference_params\n",
    "            )\n",
    "\n",
    "            # Handle different output formats\n",
    "            if hasattr(step_output, 'logits'):\n",
    "                next_token_logits = step_output.logits\n",
    "            elif isinstance(step_output, tuple):\n",
    "                next_token_logits = step_output[0]\n",
    "            else:\n",
    "                # Assume it's the logits tensor directly\n",
    "                next_token_logits = step_output\n",
    "\n",
    "    # Sample next token\n",
    "    next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "    generated_tokens.append(next_token.item())\n",
    "\n",
    "    # Ensure current_token is a scalar for next iteration\n",
    "    current_token = next_token.squeeze()  # This should be a scalar\n",
    "\n",
    "    # Update seqlen_offset after each generated token\n",
    "    inference_params.seqlen_offset += 1\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        token_text = tokenizer.decode(next_token.item())\n",
    "        print(f\"Step {step + 1}: token {next_token.item()} = '{tokenizer.decode(next_token.item())}'\")\n",
    "\n",
    "    # Check if we hit an end token\n",
    "    if next_token.item() in [tokenizer.eos_token_id, 0]:\n",
    "        print(f\"  End token reached at step {step + 1}\")\n",
    "        break\n",
    "\n",
    "total_decode_time = time.time() - decode_start_time\n",
    "\n",
    "print(f\"\\n✓ Generated {len(generated_tokens)} tokens\")\n",
    "print(f\"Total decode time: {total_decode_time*1000:.2f} ms\")\n",
    "print(f\"Time per token: {total_decode_time/len(generated_tokens)*1000:.2f} ms/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RESULTS ===\n",
      "Input: 'This paper addresses the challenges of running multiple machine learningmodels on resource-constrained edge devices, which are often equipped with a variety of processors like CPUs, GPUs, and DSPs. The primary goal is'\n",
      "Generated: ' to develop a framework that can efficiently run multiple machine learning models on edge devices. The framework is based'\n",
      "\n",
      "✅ Mamba cache API test successful!\n",
      "✅ All 3 tests passed:\n",
      "   1. Prefill trunk_1 ✓\n",
      "   2. Continue with trunk_2 ✓\n",
      "   3. Decode from cache ✓\n"
     ]
    }
   ],
   "source": [
    "# Combine and decode\n",
    "full_input = tokenizer.decode(input_ids_1[0]) + tokenizer.decode(input_ids_2[0])\n",
    "generated_text = tokenizer.decode(generated_tokens)\n",
    "\n",
    "print(\"=== FINAL RESULTS ===\")\n",
    "print(f\"Input: '{full_input}'\")\n",
    "print(f\"Generated: '{generated_text}'\")\n",
    "print(f\"\\n✅ Mamba cache API test successful!\")\n",
    "print(f\"✅ All 3 tests passed:\")\n",
    "print(f\"   1. Prefill trunk_1 ✓\")\n",
    "print(f\"   2. Continue with trunk_2 ✓\")\n",
    "print(f\"   3. Decode from cache ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
