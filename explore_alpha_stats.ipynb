{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310ab78",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285de11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest experiment directory\n",
    "exp_dir = './experiments'\n",
    "subdirs = [d for d in os.listdir(exp_dir) if d.startswith('alpha_stats')]\n",
    "latest_subdir = sorted(subdirs)[-1]\n",
    "output_dir = os.path.join(exp_dir, latest_subdir)\n",
    "\n",
    "print(f\"Loading data from: {output_dir}\")\n",
    "\n",
    "# List all files\n",
    "files = [f for f in os.listdir(output_dir) if f.endswith('.pt')]\n",
    "print(f\"Found {len(files)} files\")\n",
    "print(f\"Files: {files[:5]}...\" if len(files) > 5 else f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67eb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first file to inspect\n",
    "first_file = os.path.join(output_dir, files[0])\n",
    "data = torch.load(first_file)\n",
    "\n",
    "print(\"Data structure:\")\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {list(value.shape)} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685c246",
   "metadata": {},
   "source": [
    "## 2. Visualize Alpha Mean - Heatmaps\n",
    "\n",
    "Visualize how attention weights vary across layers and sequence positions for different heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad239ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mean = data['alpha_mean']  # [n_layer, seqlen, nheads, headdim]\n",
    "alpha_var = data['alpha_var']\n",
    "\n",
    "n_layer, seqlen, nheads, headdim = alpha_mean.shape\n",
    "print(f\"Shape: [layers={n_layer}, seqlen={seqlen}, heads={nheads}, headdim={headdim}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f99e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across head dimensions to get [n_layer, seqlen, nheads]\n",
    "alpha_mean_avg = alpha_mean.mean(dim=-1).numpy()  # [n_layer, seqlen, nheads]\n",
    "\n",
    "# Select random heads to visualize\n",
    "np.random.seed(42)\n",
    "selected_heads = np.random.choice(nheads, min(6, nheads), replace=False)\n",
    "print(f\"Visualizing heads: {selected_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eff88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmaps for selected heads\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, head_idx in enumerate(selected_heads):\n",
    "    # Get data for this head: [n_layer, seqlen]\n",
    "    head_data = alpha_mean_avg[:, :, head_idx]\n",
    "    \n",
    "    im = axes[idx].imshow(head_data, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    axes[idx].set_xlabel('Sequence Position')\n",
    "    axes[idx].set_ylabel('Layer')\n",
    "    axes[idx].set_title(f'Alpha Mean - Head {head_idx}')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_alpha_mean_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_alpha_mean_heatmaps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995dcc10",
   "metadata": {},
   "source": [
    "## 3. Visualize Alpha Mean - Line Plots\n",
    "\n",
    "Plot how attention weights change across sequence positions for selected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195374bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific layers to visualize\n",
    "if n_layer >= 24:\n",
    "    selected_layers = [0, 5, 11, 17, 23]  # Early, middle, late\n",
    "elif n_layer >= 12:\n",
    "    selected_layers = [0, n_layer//4, n_layer//2, 3*n_layer//4, n_layer-1]\n",
    "else:\n",
    "    selected_layers = [0, n_layer//2, n_layer-1]\n",
    "\n",
    "print(f\"Visualizing layers: {selected_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2499f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot line plots for selected heads\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, head_idx in enumerate(selected_heads):\n",
    "    for layer in selected_layers:\n",
    "        # Get data for this head and layer: [seqlen]\n",
    "        layer_data = alpha_mean_avg[layer, :, head_idx]\n",
    "        axes[idx].plot(layer_data, label=f'Layer {layer}', alpha=0.7)\n",
    "    \n",
    "    axes[idx].set_xlabel('Sequence Position')\n",
    "    axes[idx].set_ylabel('Alpha Mean')\n",
    "    axes[idx].set_title(f'Head {head_idx} - Selected Layers')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_alpha_mean_lines.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_alpha_mean_lines.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f846ecd",
   "metadata": {},
   "source": [
    "## 4. Visualize Alpha Variance\n",
    "\n",
    "Variance shows how consistent the attention patterns are across different doc1 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e72d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average variance across head dimensions\n",
    "alpha_var_avg = alpha_var.mean(dim=-1).numpy()  # [n_layer, seqlen, nheads]\n",
    "\n",
    "# Plot variance heatmaps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, head_idx in enumerate(selected_heads):\n",
    "    # Get data for this head: [n_layer, seqlen]\n",
    "    head_data = alpha_var_avg[:, :, head_idx]\n",
    "    \n",
    "    im = axes[idx].imshow(head_data, aspect='auto', cmap='hot', interpolation='nearest')\n",
    "    axes[idx].set_xlabel('Sequence Position')\n",
    "    axes[idx].set_ylabel('Layer')\n",
    "    axes[idx].set_title(f'Alpha Variance - Head {head_idx}')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_alpha_var_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_alpha_var_heatmaps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35baca3",
   "metadata": {},
   "source": [
    "## 5. Analyze Attention Patterns\n",
    "\n",
    "Analyze which tokens receive the most attention from the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658a6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across all heads and headdims: [n_layer, seqlen]\n",
    "alpha_overall = alpha_mean.mean(dim=(2, 3)).numpy()\n",
    "\n",
    "# Plot overall attention pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap\n",
    "im = axes[0].imshow(alpha_overall, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_xlabel('Sequence Position')\n",
    "axes[0].set_ylabel('Layer')\n",
    "axes[0].set_title('Overall Alpha Mean (averaged across all heads)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Line plot for selected layers\n",
    "for layer in selected_layers:\n",
    "    axes[1].plot(alpha_overall[layer, :], label=f'Layer {layer}', alpha=0.7, linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Sequence Position')\n",
    "axes[1].set_ylabel('Alpha Mean')\n",
    "axes[1].set_title('Overall Attention Pattern - Selected Layers')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_overall_pattern.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_overall_pattern.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f91c04",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attention Comparison\n",
    "\n",
    "Compare attention patterns across different heads for a specific layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a middle layer\n",
    "target_layer = n_layer // 2\n",
    "print(f\"Analyzing layer {target_layer}\")\n",
    "\n",
    "# Get data for this layer: [seqlen, nheads, headdim]\n",
    "layer_alpha = alpha_mean[target_layer].numpy()\n",
    "layer_alpha_avg = layer_alpha.mean(axis=-1)  # [seqlen, nheads]\n",
    "\n",
    "# Plot heatmap showing all heads\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap: heads vs sequence position\n",
    "im = axes[0].imshow(layer_alpha_avg.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_xlabel('Sequence Position')\n",
    "axes[0].set_ylabel('Head Index')\n",
    "axes[0].set_title(f'Layer {target_layer} - All Heads')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Select some heads to plot as lines\n",
    "heads_to_plot = np.linspace(0, nheads-1, min(8, nheads), dtype=int)\n",
    "for head in heads_to_plot:\n",
    "    axes[1].plot(layer_alpha_avg[:, head], label=f'Head {head}', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Sequence Position')\n",
    "axes[1].set_ylabel('Alpha Mean')\n",
    "axes[1].set_title(f'Layer {target_layer} - Selected Heads')\n",
    "axes[1].legend(fontsize=8, ncol=2)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_multihead_layer{target_layer}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_multihead_layer{target_layer}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194412e",
   "metadata": {},
   "source": [
    "## 7. Layer-wise Statistics\n",
    "\n",
    "Analyze how attention patterns evolve across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for each layer\n",
    "layer_mean = alpha_overall.mean(axis=1)  # Average attention per layer\n",
    "layer_std = alpha_overall.std(axis=1)    # Variability per layer\n",
    "layer_max = alpha_overall.max(axis=1)    # Max attention per layer\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Mean attention\n",
    "axes[0].plot(layer_mean, marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Mean Alpha')\n",
    "axes[0].set_title('Average Attention per Layer')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Standard deviation\n",
    "axes[1].plot(layer_std, marker='o', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Std Alpha')\n",
    "axes[1].set_title('Attention Variability per Layer')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Max attention\n",
    "axes[2].plot(layer_max, marker='o', linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Layer')\n",
    "axes[2].set_ylabel('Max Alpha')\n",
    "axes[2].set_title('Maximum Attention per Layer')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_layer_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_layer_statistics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da179f3",
   "metadata": {},
   "source": [
    "## 8. Compare Multiple doc2 Samples\n",
    "\n",
    "Load and compare attention patterns from different doc2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60307bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple files\n",
    "num_files_to_compare = min(4, len(files))\n",
    "selected_files = files[:num_files_to_compare]\n",
    "\n",
    "print(f\"Comparing {num_files_to_compare} doc2 samples\")\n",
    "\n",
    "# Load data from each file\n",
    "all_alphas = []\n",
    "doc_ids = []\n",
    "for f in selected_files:\n",
    "    filepath = os.path.join(output_dir, f)\n",
    "    data_tmp = torch.load(filepath)\n",
    "    # Average across heads and headdims: [n_layer, seqlen]\n",
    "    alpha_tmp = data_tmp['alpha_mean'].mean(dim=(2, 3)).numpy()\n",
    "    all_alphas.append(alpha_tmp)\n",
    "    doc_ids.append(data_tmp['doc2_id'][:8])\n",
    "\n",
    "print(f\"Loaded doc2 IDs: {doc_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf290fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (alpha_data, doc_id) in enumerate(zip(all_alphas, doc_ids)):\n",
    "    im = axes[idx].imshow(alpha_data, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "    axes[idx].set_xlabel('Sequence Position')\n",
    "    axes[idx].set_ylabel('Layer')\n",
    "    axes[idx].set_title(f'Doc2: {doc_id}')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}_doc2_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_dir}_doc2_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d51d8",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d28718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nExperiment: {output_dir}\")\n",
    "print(f\"Number of doc2 samples: {len(files)}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Doc2 ID: {data['doc2_id']}\")\n",
    "print(f\"  Number of doc1 paired: {data['num_doc1_samples']}\")\n",
    "print(f\"  Max sequence length: {data['max_seqlen']}\")\n",
    "print(f\"  Model architecture: {data['n_layer']} layers, {data['nheads']} heads, {data['headdim']} headdim\")\n",
    "print(f\"\\nAlpha statistics:\")\n",
    "print(f\"  Mean range: [{alpha_mean.min():.4f}, {alpha_mean.max():.4f}]\")\n",
    "print(f\"  Mean average: {alpha_mean.mean():.4f}\")\n",
    "print(f\"  Variance range: [{alpha_var.min():.4f}, {alpha_var.max():.4f}]\")\n",
    "print(f\"  Variance average: {alpha_var.mean():.4f}\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
